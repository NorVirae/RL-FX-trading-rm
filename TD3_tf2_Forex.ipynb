{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6df327c",
   "metadata": {},
   "source": [
    "##### <br>***Code written in this notebook is in reference to the  expanation paper 'Reinforcement Learning in Foreign Exchange Trading' in the same directory***<br>  \n",
    "\n",
    "This notebook is for training, validating, and testing new models.  \n",
    "- To use the pre-trained model for daily prediction, please refer to the **Pre_trained_implementation** notebook.\n",
    "- To use the pre-trained model to backtesting its performance, please refer to the **Pre_trained_Model_Performance** notebook.\n",
    "- The algorithm can be train to trade any currency pair or a portfolio of currency pairs<br><br>  \n",
    "\n",
    "Before training a new Model, user can decide:\n",
    "- Whether to use the same price indicators as proposed to build an agent-observable dataset.  \n",
    "- Whether to import any pre-acquired agent-observable dataset.<br><br>  \n",
    "\n",
    "This notebook can be seperated into parts of:  \n",
    "- Training  \n",
    "- Validation  \n",
    "- Testing  \n",
    "- Implementation  \n",
    "- Details<br>\n",
    "\n",
    "where inputs from the user are required are tagged with `[User-input required]`\n",
    "<br><br>\n",
    "\n",
    "### Content  \n",
    "\n",
    "1.[Define Historical Price Dataset](#def_bid_ask) <font size=3>*[Training and Validation]*</font>`[User-input required]`   \n",
    "\n",
    "2.[Define Agent-observable Dataset](#def_obs) <font size=3>*[Training and Validation]*</font>`[User-input required]`  \n",
    "\n",
    "3.[Extract Price Indicators](#extract) <font size=3>*[Details]*</font>  \n",
    "\n",
    "4.[Preprocess](#prep) <font size=3>*[Details]*</font>   \n",
    "\n",
    "5.[Define Train & Validation Data](#def_train_val) <font size=3>*[Training and Validation]*</font>  \n",
    "\n",
    "6.[Experience Replay Buffer](#replay) <font size=3>*[Details]*</font>  \n",
    "\n",
    "7.[Actor Network & Critic Network](#ac_cr) <font size=3>*[Details]*</font>  \n",
    "\n",
    "8.[Twin Delayed Deep Deterministic Policy Gradients](#td3) <font size=3>*[Details]*</font>   \n",
    "\n",
    "9.[Forex Spread-Betting Environment](#env) <font size=3>*[Details]*</font>  \n",
    "\n",
    "10.[Set hypermarameters](#hparms) <font size=3>*[Training and Validation]*</font>`[User-input optional]`  \n",
    "\n",
    "11.[Training and Validation](#train) <font size=3>*[Training and Validation]*</font>  \n",
    "\n",
    "12.[Agent-Iteration Testing](#test) <font size=3>*[Testing]*</font>`[User-input required]`  \n",
    "\n",
    "13.[Simple implementation within the notebook](#simple_imp) <font size=3>*[Implementation]*</font>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4370d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from tqdm.notebook import trange\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (16,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69adc46b",
   "metadata": {},
   "source": [
    "### Define Historical Price Dataset  <a name=\"def_bid_ask\"></a>\n",
    "#### [Training and Validation] \n",
    "#### **(User-input required)**  \n",
    "\n",
    "Historical price dataset, ```historical_data```, an unprocessed price \n",
    "dataset that must contain prices for defining \n",
    "trade entries and exits for reward calculation. (pandas DataFrame expected)<br>  \n",
    "e.g. ```historical_data = pd.read_csv('...')```  \n",
    "\n",
    "<br><br>\n",
    "- ```historical_data.shape``` is expected to be ```(timestamps, bid_ask_prices)```,  \n",
    "where the columns must contain at least one bid price and one ask price for \n",
    "defining trade entries and exits.<br>  \n",
    "\n",
    "    The historical price data is generally expected to be have index and columns as shown:\n",
    "\n",
    "    | Index | Bid Open | Bid High | Bid Low | Bid Close | Ask Open | Ask High | Ask Low | Ask Close |\n",
    "    | :---: | :------: | :------: | :-----: | :-------: | :------: | :------: | :-----: | :-------: |\n",
    "    | Timestamp_0 | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "    | Timestamp_1 | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "    | Timestamp_2 | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "    |     ...     | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "    | Timestamp_n | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "<br><br>\n",
    "- ```historical_data.columns``` are expected to be \n",
    "```['Bid Open', 'Bid High', 'Bid Low', 'Bid Close', 'Ask Open', 'Ask High', 'Ask Low', 'Ask Close']```  \n",
    "    - If different, specify the columns to use for entries and exits when defining \n",
    "    ```ForexEnv``` (trading environment) by setting the [hyperparameters (bid_col, ask_col)](#hparms), \n",
    "    and details can be found [here](#env)<br>  \n",
    "        e.g. If ```historical_data.columns``` are ```['Bid Close', Ask Close']```, \n",
    "        then ```bid_col = 0```, ```ask_col = 1```.<br><br>      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (user-input)\n",
    "historical_data = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10701ba8",
   "metadata": {},
   "source": [
    "### Define Agent-observable Dataset  <a name=\"def_obs\"></a>\n",
    "#### [Training and Validation]\n",
    "#### **(User-input required)**   \n",
    "An agent-observable dataset, ```agent_observable_data```,  is what the learning \n",
    "algorithm can observe at each timestamp. It can be dependent on \n",
    "the particular currency the algorithm learns to trade.  \n",
    "\n",
    "<br><br>\n",
    "There are two ways of defining it:<br>  \n",
    "\n",
    "1. By default we build an agent-observable dataset from the user-provided unprocessed historical price \n",
    "dataset (```historical_data```). We extract some technical indicators (same indicators as proposed in the paper) \n",
    "from the user's dataset, which then becomes our agent-observable dataset.\n",
    "    - The ```historical_data.columns``` must contains candlesticks data, e.g. BidOpen, BidHigh, BidLow, BidClose of a price.\n",
    "    - User can still decide the training portion of the dataset by defining ```user_train_portion```, default 0.8.\n",
    "    - To build an agent-observable dataset using the same price indicators but with different parameters, \n",
    "    refer to ```build_agent_obs_dataset()```under [Preprocess](#prep)\n",
    "    - To disable, Set ```build_agent_obs_off_users = False```<br><br>  \n",
    "\n",
    "2. User can import custom agent-observable dataset by either<br>  \n",
    "    I. specifying ```agent_observable_data``` and ```user_train_portion``` to define the training \n",
    "    and validation portions of the datasets, e.g.  \n",
    "    ```agent_observable_data = pd.read_csv(\"...\")```  \n",
    "    ```user_train_portion = 0.8```<br><br>  \n",
    "    \n",
    "    II. or specifying ```train_agent_obs``` & ```val_agent_obs```, e.g.  \n",
    "    ```train_agent_obs = pd.read_csv(\"...\")```  \n",
    "    ```val_agent_obs = pd.read_csv(\"...\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Whether or not to build an agent-observable dataset from the user-provided historical_data\n",
    "build_agent_obs_off_users = True\n",
    "\n",
    "# 2I. Define an agent-observable dataset and the train portion to represent the market states \n",
    "# (pandas dataframe expected)\n",
    "agent_observable_data = None\n",
    "user_train_portion = None  \n",
    "\n",
    "# 2II. Define the training and vaidation agent-observation dataset\n",
    "train_agent_obs = None    \n",
    "val_agent_obs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676cb5e",
   "metadata": {},
   "source": [
    "### Extract Price Indicators <a name='extract'></a>\n",
    "#### [Details]<br>  \n",
    "\n",
    "Build an agent-observable dataset with the same set of variables used in the paper, \n",
    "from a price dataset that contains columns of ```['Open', 'High', 'Low', 'Close']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma(df, ma_ranges=[10, 21, 50]):\n",
    "    \"\"\"\n",
    "    Simple Moving Average\n",
    "    \n",
    "    df : pandas.DataFrame, must include columns ['Close']\n",
    "\n",
    "    ma_ranges: list, default [10, 21, 50]\n",
    "    List of periods of Simple Moving Average to be extracted\n",
    "    \"\"\"\n",
    "        \n",
    "    df = df.copy()\n",
    "    for period in ma_ranges:\n",
    "        df[f\"MA{period}\"] = df['Close'].rolling(window=period).mean()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macd(df, short_long_signals=[(12,26,9)]):\n",
    "    \"\"\"\n",
    "    Moving Average Convergence Divergence\n",
    "    \n",
    "    df : pandas.DataFrame, must include columns ['Close']\n",
    "\n",
    "    short_long_signals : list, default [(12, 26, 9)] \n",
    "    List of periods of (short_ema, long_ema, signal) of Moving Average Convergence Divergence to be extracted\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    for (short, long, signal) in short_long_signals:\n",
    "        df[f\"EMA{short}\"] = df[\"Close\"].ewm(span=short, adjust=False).mean()\n",
    "        df[f\"EMA{long}\"] = df[\"Close\"].ewm(span=long, adjust=False).mean()\n",
    "        df[f\"MACD{long}\"] = df[f\"EMA{short}\"] - df[f\"EMA{long}\"]\n",
    "        df[f\"MACD{long}Signal\"] = df[f\"MACD{long}\"].ewm(span=signal, adjust=False).mean()\n",
    "        df = df.drop(columns=[f\"EMA{short}\", f\"EMA{long}\"])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f70732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_stochastic(df, stochastic_ranges=[(14,3,3)]):\n",
    "    \"\"\"\n",
    "    Full Stochastic Indicator\n",
    "    \n",
    "    df : pandas.DataFrame, must include columns ['High', 'Low', 'Close']\n",
    "\n",
    "    stochastic_ranges : list, default [(14, 3, 3)]\n",
    "    List of periods of (fast_k, fast_d, slow_d) of Full Stochastic Indicator to be extracted\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    for (fast_k, fast_d, slow_d) in stochastic_ranges:\n",
    "        df[f\"L{fast_k}\"] = df[\"Low\"].rolling(window=fast_k).min()\n",
    "        df[f\"H{fast_k}\"] = df[\"High\"].rolling(window=fast_k).max()\n",
    "        df[f\"fast_%K{fast_k}\"] = 100*((df[\"Close\"] - df[f\"L{fast_k}\"])\n",
    "                                                 /(df[f\"H{fast_k}\"] - df[f\"L{fast_k}\"]))\n",
    "        df[f\"full_%K{fast_k}_fast_%D{fast_d}\"] = df[f\"fast_%K{fast_k}\"].rolling(window=fast_d).mean()\n",
    "        df[f\"full_%K{fast_k}_slow_%D{slow_d}\"] = df[f\"full_%K{fast_k}_fast_%D{fast_d}\"].rolling(window=slow_d).mean()\n",
    "        df = df.drop(columns=[f\"L{fast_k}\", f\"H{fast_k}\", f\"fast_%K{fast_k}\"])\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    " def rsi(df, rsi_ranges=[14]):\n",
    "    \"\"\"\n",
    "    Relative Strength Index\n",
    "    \n",
    "    df : pandas.DataFrame, must include columns ['Open', 'Close]\n",
    "\n",
    "    rsi_ranges: list, default [14]\n",
    "    List of periods of rsi to be extracted\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"Up_Close\"] = np.where((df[\"Close\"] > df[\"Open\"]), df[\"Close\"], 0)\n",
    "    df[\"Down_Close\"] = np.where((df[\"Close\"] < df[\"Open\"]), df[\"Close\"], 0)\n",
    "    for period in rsi_ranges:\n",
    "        df[f\"RS{period}_RollUpAvg\"] = df[\"Up_Close\"].ewm(span=period, adjust=False).mean()\n",
    "        df[f\"RS{period}_RollDownAvg\"] = df[\"Down_Close\"].ewm(span=period, adjust=False).mean()\n",
    "        df[f\"RSI{period}\"] = 100 - (100 / (1 + (df[f\"RS{period}_RollUpAvg\"] / df[f\"RS{period}_RollDownAvg\"])))\n",
    "        df = df.drop(columns=[f\"RS{period}_RollUpAvg\", f\"RS{period}_RollDownAvg\"])\n",
    "    df = df.drop(columns=[\"Up_Close\", \"Down_Close\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aeba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bollinger_bands(df, bollinger_period_sd_ranges=[(20,2)]):\n",
    "    \"\"\"\n",
    "    Bollinger Bands (including %B and Bandwidth)\n",
    "    \n",
    "    df : pandas.DataFrame, must include columns ['High', 'Low', 'Close]\n",
    "\n",
    "    bollinger_period_sd_ranges : list, default [(20,2)]\n",
    "    List of (period, standard_deviation) to be extracted\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"TypicalPrice\"] = ((df[\"High\"] + df[\"Low\"] + df[\"Close\"]) / 3)\n",
    "    for (period, sd) in bollinger_period_sd_ranges:\n",
    "        ma_typicalPrice = df[\"TypicalPrice\"].rolling(window=period).mean()\n",
    "        sd_typicalPrice = ma_typicalPrice.rolling(window=period).std(ddof=1)\n",
    "        df[f\"UpperBollinger{period}\"] = ma_typicalPrice + sd * sd_typicalPrice\n",
    "        df[f\"LowerBollinger{period}\"] = ma_typicalPrice - sd * sd_typicalPrice\n",
    "        df[f\"%B{period}\"] = ((df[\"Close\"] - df[f\"LowerBollinger{period}\"]) / \n",
    "                             (df[f\"UpperBollinger{period}\"] - df[f\"LowerBollinger{period}\"]))\n",
    "        df[f\"Bandwidth{period}\"] = (df[f\"UpperBollinger{period}\"] - df[f\"LowerBollinger{period}\"]) / ma_typicalPrice\n",
    "    df = df.drop(columns=[f\"TypicalPrice\"])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag(df, lag=5):\n",
    "    \"\"\"\n",
    "    Add lags to dataset to provide historical context.\n",
    "    \n",
    "    df : pandas.DataFrame\n",
    "\n",
    "    lag: int, default 5\n",
    "    Number of lags to be added\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    cols = list(df.columns)\n",
    "    cols_len = len(cols)\n",
    "    for i in range(1, lag+1):\n",
    "        df = pd.concat([df, df.iloc[:,:cols_len].shift(i)], axis=1)\n",
    "        cols.extend([x + f\"n-{i}\" for x in df.columns[:cols_len]])\n",
    "        df.columns = cols\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6207d5",
   "metadata": {},
   "source": [
    "### Preprocess <a name='prep'></a>\n",
    "#### [Details]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e67dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df, one_side_remove_percentile=0.005, train_portion=0.8):\n",
    "    \"\"\"\n",
    "    Remove the outliers of each column of the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Dataframe to remove outliers from\n",
    "\n",
    "    one_side_remove_percentile : The percentile of outliers to be removed on each end, default 0.005\n",
    "\n",
    "    train_portion : The percentage of dataset used for training, default 0.8\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    df : pandas.DataFrame, Processed dataframe\n",
    "\n",
    "    mins : numpy.array, Min values of each columns\n",
    "\n",
    "    maxs : numpy.array, Max values of each columns\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    mins, maxs = [], []\n",
    "    df_cols = df.columns\n",
    "    df_ind = df.index\n",
    "    df = np.array(df)\n",
    "    for i in range(df.shape[1]):\n",
    "        temp = df[:,i]\n",
    "        mins.append(np.sort(temp[:int(df.shape[0]*train_portion)])[int(df.shape[0]*one_side_remove_percentile)])\n",
    "        maxs.append(np.sort(temp[:int(df.shape[0]*train_portion)])[-int(df.shape[0]*one_side_remove_percentile)])\n",
    "    df = np.clip(df, mins, maxs)\n",
    "    df = pd.DataFrame(df, columns=df_cols, index=df_ind)\n",
    "    \n",
    "    return df, np.array(mins), np.array(maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmaxscaler(df, mins, maxs):\n",
    "    \"\"\"\n",
    "    Rescale the data of the dataset using MinMaxScaler given the min and max values of each column.\n",
    "    \n",
    "    df : pandas.DataFrame\n",
    "\n",
    "    mins : Min values of each columns\n",
    "\n",
    "    maxs : Max values of each columns\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df = (df - mins) / (maxs - mins)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent_obs_dataset(unprocessed_df,\n",
    "                            rename_columns=['Open', 'High', 'Low', 'Close', 'AskOpen', 'AskHigh', 'AskLow', 'AskClose'],\n",
    "                            ma_ranges=[10, 21, 50],\n",
    "                            short_long_signals=[(12, 26, 9)],\n",
    "                            stochastic_ranges=[(14, 3, 3)],\n",
    "                            rsi_ranges=[14],\n",
    "                            bollinger_period_sd_ranges=[(20, 2)],\n",
    "                            lag=5,\n",
    "                            one_side_remove_percentile=0.005,\n",
    "                            train_portion=0.8):\n",
    "    \"\"\"\n",
    "    Build an agent-observable dataset that represents states the agent encounters at each timestamp.\n",
    "    The default parameters are in reference to the ones used in the explanation paper.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unprocessed_df : pandas.DataFrame\n",
    "     Dataframe where the price indicators are extracted must contains candlestick data.\n",
    "     e.g. columns = ['BidOpen', 'BidHigh', 'BidLow', 'BidClose', 'AskOpen', 'AskHigh', 'AskLow', 'AskClose']\n",
    "\n",
    "    rename_columns : list, default ['Open', 'High', 'Low', 'Close', 'AskOpen', 'AskHigh', 'AskLow', 'AskClose']\n",
    "     (Default price indicators are extracted from bid prices)\n",
    "     Rename columns to include ['Open', 'High', 'Low', 'Close'], where the price indicators are extracted from. \n",
    "     If they already exist, simply pass on the unprocessed dataframe columns. (unprocessed_df.columns)\n",
    "\n",
    "    ma_ranges : list, default [10, 21, 50]\n",
    "     List of periods of simple moving average to be extracted\n",
    "\n",
    "    short_long_signals : list, default [(12, 26, 9)]\n",
    "     List of periods (short_ema, long_ema, signal) of Moving Average Convergence Divergence to be extracted\n",
    "\n",
    "    stochastic_ranges : list, default [(14, 3, 3)]\n",
    "     List of periods (fast_k, fast_d, slow_d) of Full Stochastic Indicator to be extracted\n",
    "\n",
    "    rsi_ranges : list, default [14]\n",
    "     List of periods of Relative Strength Index to be extracted\n",
    "\n",
    "    bollinger_period_sd_ranges : list, default [(20,2)]\n",
    "     List of (period, standard_deviation) of Bollinger Bands (including %B and Bandwidth) to be extracted\n",
    "\n",
    "    lag : int, default 5\n",
    "     Number of lags added to the dataset to provide historical context\n",
    "\n",
    "    one_side_remove_percentile : float, default 0.005\n",
    "     The percentile of outliers to be removed on each end\n",
    "\n",
    "    train_portion : float, default 0.8\n",
    "     The percentage of dataset used for training\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_agent_obs : pandas.DataFrame\n",
    "     Agent-observable dataset for training\n",
    "\n",
    "    val_agent_obs : pandas.DataFrame\n",
    "     Agent-observable dataset for validation\n",
    "    \"\"\"\n",
    "\n",
    "    agent_obs_data = unprocessed_df.copy()\n",
    "    agent_obs_data.columns = rename_columns\n",
    "    agent_obs_data = ma(df=agent_obs_data, ma_ranges=ma_ranges)\n",
    "    agent_obs_data = macd(df=agent_obs_data, short_long_signals=short_long_signals)\n",
    "    agent_obs_data = full_stochastic(df=agent_obs_data, stochastic_ranges=stochastic_ranges)\n",
    "    agent_obs_data = rsi(df=agent_obs_data, rsi_ranges=rsi_ranges)\n",
    "    agent_obs_data = bollinger_bands(df=agent_obs_data, bollinger_period_sd_ranges=bollinger_period_sd_ranges)\n",
    "    agent_obs_data = agent_obs_data.iloc[:, len(rename_columns):]\n",
    "    agent_obs_data = add_lag(df=agent_obs_data, lag=lag)\n",
    "    agent_obs_data = agent_obs_data.dropna()\n",
    "\n",
    "    agent_obs_data, mins, maxs = remove_outlier(agent_obs_data, one_side_remove_percentile=one_side_remove_percentile,\n",
    "                                                train_portion=train_portion)\n",
    "\n",
    "    # We allow Upper and Lower BollingerBands to share the same mins and maxs\n",
    "    mins[np.where(['UpperBollinger' in x for x in list(agent_obs_data.columns)])[0]] = mins[\n",
    "        np.where(['LowerBollinger' in x for x in list(agent_obs_data.columns)])[0]]\n",
    "    maxs[np.where(['LowerBollinger' in x for x in list(agent_obs_data.columns)])[0]] = maxs[\n",
    "        np.where(['UpperBollinger' in x for x in list(agent_obs_data.columns)])[0]]\n",
    "\n",
    "    # We do not specify train_portion here as the mins and maxs are extracted from the training set\n",
    "    agent_obs_data = minmaxscaler(agent_obs_data, mins, maxs)\n",
    "\n",
    "    train_agent_obs = agent_obs_data.iloc[:int(agent_obs_data.shape[0] * train_portion)]\n",
    "    val_agent_obs = agent_obs_data.iloc[int(agent_obs_data.shape[0] * train_portion):]\n",
    "    \n",
    "    return train_agent_obs, val_agent_obs, mins, maxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb4ec70",
   "metadata": {},
   "source": [
    "### Define Train & Validation Data  <a name='def_train_val'></a>  \n",
    "#### [Training and Validation]<br>  \n",
    "\n",
    "If ```historical_data.columns``` is not ```['BidOpen', 'BidHigh', 'BidLow', 'BidClose', 'AskOpen', 'AskHigh', 'AskLow', 'AskClose']```,  \n",
    "\n",
    "- read the docstring in ```build_agent_obs_dataset()``` for the ```rename_columns``` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1766dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (train_agent_obs is None) & (val_agent_obs is None):\n",
    "    if agent_observable_data is not None:\n",
    "        if user_train_portion is None:\n",
    "            print(\"Did not specify train portion, train portion will be set to 80%\")\n",
    "            user_train_portion = 0.8\n",
    "        train_agent_obs = agent_observable_data.iloc[:int(agent_observable_data.shape[0]*user_train_portion)]   \n",
    "        train_historical = historical_data.loc[train_agent_obs.index]\n",
    "        \n",
    "        val_agent_obs = agent_observable_data.iloc[int(agent_observable_data.shape[0]*user_train_portion):]\n",
    "        val_historical = historical_data.loc[val_agent_obs.index]\n",
    "        \n",
    "    elif build_agent_obs_off_users:\n",
    "        if user_train_portion is not None:\n",
    "            (train_agent_obs, val_agent_obs, \n",
    "             scaler_mins, scaler_maxs) = build_agent_obs_dataset(historical_data, train_portion=user_train_portion)   \n",
    "        else:\n",
    "            print(\"Did not specify train portion, train portion will be set to 80%\")\n",
    "            (train_agent_obs, val_agent_obs, \n",
    "             scaler_mins, scaler_maxs) = build_agent_obs_dataset(historical_data)\n",
    "            \n",
    "        train_historical = historical_data.loc[train_agent_obs.index]\n",
    "        val_historical = historical_data.loc[val_agent_obs.index]\n",
    "        \n",
    "elif (train_agent_obs is not None) & (val_agent_obs is not None):\n",
    "    train_historical = historical_data.loc[train_agent_obs.index]\n",
    "    val_historical = historical_data.loc[val_agent_obs.index]\n",
    "\n",
    "# Agent-observable dataset in training\n",
    "train_agent_obs = np.array(train_agent_obs)\n",
    "\n",
    "# Unprocessed price dataset for reward calculation in training\n",
    "train_historical = np.array(train_historical)\n",
    "\n",
    "# Agent-observable dataset in validation\n",
    "val_agent_obs = np.array(val_agent_obs)\n",
    "\n",
    "# Unprocess price dataset for reward calculation in validation\n",
    "val_historical = np.array(val_historical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6695a9f",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer <a name='replay'></a>\n",
    "#### [Details]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fba435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, input_shape, n_actions): \n",
    "        self.mem_size = max_size # The maximum size of the replay buffer\n",
    "        self.mem_counter = 0 \n",
    "        self.state_memory = np.empty((self.mem_size, *input_shape)) * np.nan\n",
    "        self.new_state_memory = np.empty((self.mem_size, *input_shape)) * np.nan\n",
    "        self.action_memory = np.empty((self.mem_size, n_actions)) * np.nan\n",
    "        self.reward_memory = np.empty(self.mem_size) * np.nan\n",
    "        self.terminal_memory = np.empty(self.mem_size, dtype=bool) * np.nan\n",
    "        \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        \"\"\"\n",
    "        Store an experience into the experience replay buffer\n",
    "        \"\"\"\n",
    "        \n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = new_state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done \n",
    "        self.mem_counter += 1\n",
    "        \n",
    "    def sample_buffer(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample experience batches to train our model\n",
    "        \"\"\"\n",
    "\n",
    "        current_mem_size = min(self.mem_counter, self.mem_size)\n",
    "        batch = np.random.choice(current_mem_size, batch_size, replace=False)\n",
    "        state_batch = tf.convert_to_tensor(self.state_memory[batch])\n",
    "        next_state_batch = tf.convert_to_tensor(self.new_state_memory[batch])\n",
    "        action_batch = tf.convert_to_tensor(self.action_memory[batch])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_memory[batch])\n",
    "        done_batch = tf.convert_to_tensor(self.terminal_memory[batch])\n",
    "        \n",
    "        return state_batch, next_state_batch, action_batch, reward_batch, done_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc09f85",
   "metadata": {},
   "source": [
    "### Actor Network & Critic Network <a name='ac_cr'></a>\n",
    "#### [Details]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1183a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Hidden layer in the Actor network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fc_dim, activation='relu'):\n",
    "        super(ActorLayer, self).__init__()\n",
    "        \n",
    "        self.dense = layers.Dense(fc_dim, activation=activation)\n",
    "        \n",
    "    def call(self, state):\n",
    "        prob = self.dense(state)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdd757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(keras.Model):\n",
    "    \"\"\"\n",
    "    Approximation for the optimal actions given the observations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fc_dim=512, num_layers=2, activation='relu',\n",
    "                 n_actions=1, name='actor'):        \n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers       \n",
    "        self.actorlayers = [ActorLayer(fc_dim, activation) for _ in range(num_layers)]\n",
    "        self.mu = layers.Dense(n_actions, activation='tanh', \n",
    "                               kernel_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003))\n",
    "        \n",
    "    def call(self, state):\n",
    "        for i in range(self.num_layers):\n",
    "            state = self.actorlayers[i](state)\n",
    "        \n",
    "        actions = self.mu(state)\n",
    "        \n",
    "        if tf.reduce_sum(abs(actions)) == 0:   # Practically will not happen, but technically can\n",
    "            bal_allocation = tf.cast(0, tf.float32)\n",
    "        else:\n",
    "            # bal_allocation is set to account for when n_actions >= 2\n",
    "            bal_allocation = tf.cast(tf.reduce_sum(actions ** 2) / (tf.reduce_sum(abs(actions)) ** 2), tf.float32)\n",
    "        return actions * bal_allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33185977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Hidden layer in the Critic network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fc_dim, activation='relu'):\n",
    "        super(CriticLayer, self).__init__()\n",
    "        \n",
    "        self.dense = layers.Dense(fc_dim, activation=activation)\n",
    "        \n",
    "    def call(self, state_action):\n",
    "        value = self.dense(state_action)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(keras.Model):\n",
    "    \"\"\"\n",
    "    Approximation for the Q-values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fc_dim=512, num_layers=2, activation='relu',\n",
    "                 name='critic'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.q1_criticlayers = [CriticLayer(fc_dim, activation) for _ in range(num_layers)]\n",
    "        self.q1_output_layer = layers.Dense(1, activation=None)\n",
    "        \n",
    "        self.q2_criticlayers = [CriticLayer(fc_dim, activation) for _ in range(num_layers)]\n",
    "        self.q2_output_layer = layers.Dense(1, activation=None)\n",
    "        \n",
    "    def call(self, state_input, action_input):\n",
    "        q1_state_action = layers.concatenate([state_input, action_input])\n",
    "        q2_state_action = layers.concatenate([state_input, action_input])\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            q1_state_action = self.q1_criticlayers[i](q1_state_action)\n",
    "            q2_state_action = self.q2_criticlayers[i](q2_state_action)\n",
    "            \n",
    "        q1 = self.q1_output_layer(q1_state_action)\n",
    "        q2 = self.q2_output_layer(q2_state_action)\n",
    "        return q1, q2\n",
    "    \n",
    "    def Q1(self, state_input, action_input):\n",
    "        q1_state_action = layers.concatenate([state_input, action_input])\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            q1_state_action = self.q1_criticlayers[i](q1_state_action)\n",
    "            \n",
    "        q1 = self.q1_output_layer(q1_state_action)\n",
    "        return q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ab362",
   "metadata": {},
   "source": [
    "### Twin Delayed Deep Deterministic Policy Gradients <a name='td3'></a>\n",
    "#### [Details]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3:\n",
    "    \"\"\"\n",
    "    Twin Delayed Deep Deterministic Policy Gradient\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state_dim, \n",
    "                 n_actions=1,\n",
    "                 max_action=1,\n",
    "                 min_action=-1,\n",
    "                 gamma=0.995, \n",
    "                 tau=0.005,\n",
    "                 exploration_noise=0.2, \n",
    "                 policy_noise=0.2, \n",
    "                 noise_clip=0.5, \n",
    "                 policy_freq=2, \n",
    "                 fc_dim=512, \n",
    "                 num_actor_layers=2, \n",
    "                 num_critic_layers=2,\n",
    "                 activation='relu',\n",
    "                 ac_lr=3e-4, \n",
    "                 cr_lr=3e-4,\n",
    "                 batch_size=64,\n",
    "                 max_memory_size=100000, \n",
    "                 uniform_action_steps=10000,\n",
    "                 ckpt_name=\"ckpt\", \n",
    "                 model_name=\"TD3\"):\n",
    "        \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_dim : Number of features/variables in the agent-observable dataset\n",
    "\n",
    "        n_actions : Number of actions the agent can take\n",
    "\n",
    "        max_action : Maximum action from the actor, default 1 (i.e. 100%)\n",
    "\n",
    "        min_action : Minimum action from the actor, default -1 (i.e. -100%)\n",
    "         \n",
    "        gamma : Farsightedness, how much the agent values future return, default 0.995\n",
    "        \n",
    "        tau : Target networks update rate, default 0.002\n",
    "\n",
    "        exploration_noise : Standard deviation of noise added to actor's action in training, default 0.3\n",
    "\n",
    "        policy_noise: Standard deviation of noise added to target_actor's action batch, default 0.15\n",
    "\n",
    "        noise_clip : Clip values of added noise to target_actor's action batch, default 0.3\n",
    "\n",
    "        policy_freq : Policy update frequency, default 4\n",
    "\n",
    "        fc_dim : Number of nodes in one hidden dense layer, default 1024\n",
    "\n",
    "        num_actor_layers :  Number of hidden layers in each actor networks, default 4\n",
    "\n",
    "        num_critic_layers : Number of hidden layers in each critic networks, default 4\n",
    "\n",
    "        activation : Hidden layers activation function in all actors and critics (keras dense) networks, default \"relu\"\n",
    "\n",
    "        ac_lr : Learning rate of the actors, default 1e-5\n",
    "\n",
    "        cr_lr : Learning rate of the critics, default 1e-5\n",
    "\n",
    "        batch_size : Batch size of each experience sample, default 64\n",
    "\n",
    "        max_memory_size : Maximum replay buffer memory size, default 1000000\n",
    "\n",
    "        uniform_action_steps : Steps of consecutive uniformly sampled actions in the beginning of training, default 10000\n",
    "\n",
    "        ckpt_name : Name of the checkpoint directory for model weights, default \"ckpt\"\n",
    "        \n",
    "        model_name : Name of the Model, default \"TD3\"\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = tf.cast(gamma, dtype=tf.float32)\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_memory_size, (state_dim,), n_actions)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_action = max_action\n",
    "        self.min_action = min_action\n",
    "        self.uniform_action_steps = uniform_action_steps\n",
    "        self.exploration_noise = exploration_noise\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_freq = policy_freq\n",
    "        self.total_steps = 0\n",
    "        self.ckpt_dir = f\"{model_name}/{ckpt_name}\"\n",
    "        \n",
    "        self.actor = ActorNetwork(fc_dim=fc_dim, \n",
    "                                  num_layers=num_actor_layers, \n",
    "                                  activation=activation,\n",
    "                                  n_actions=n_actions,\n",
    "                                  name=f\"actor\")\n",
    "        \n",
    "        self.target_actor = ActorNetwork(fc_dim=fc_dim, \n",
    "                                         num_layers=num_actor_layers, \n",
    "                                         activation=activation,\n",
    "                                         n_actions=n_actions,\n",
    "                                         name=f\"target_actor\")\n",
    "        \n",
    "        self.critic = CriticNetwork(fc_dim=fc_dim, \n",
    "                                    num_layers=num_critic_layers, \n",
    "                                    activation=activation,\n",
    "                                    name=f\"critic\")\n",
    "        \n",
    "        self.target_critic = CriticNetwork(fc_dim=fc_dim, \n",
    "                                           num_layers=num_critic_layers, \n",
    "                                           activation=activation,\n",
    "                                           name=f\"target_critic\")\n",
    "        \n",
    "        self.actor.compile(optimizer=Adam(learning_rate=ac_lr))\n",
    "        self.critic.compile(optimizer=Adam(learning_rate=cr_lr))\n",
    "        self.target_actor.compile(optimizer=Adam(learning_rate=ac_lr))\n",
    "        self.target_critic.compile(optimizer=Adam(learning_rate=cr_lr))\n",
    "        \n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "        \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        \"\"\"\n",
    "        Target networks weights update\n",
    "        \"\"\"\n",
    "\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "            \n",
    "        updates = [current_w*tau + target_w*(1-tau) for current_w, target_w in\n",
    "                   zip(self.actor.weights, self.target_actor.weights)]\n",
    "        self.target_actor.set_weights(updates)\n",
    "        \n",
    "        updates = [current_w*tau + target_w*(1-tau) for current_w, target_w in\n",
    "                   zip(self.critic.weights, self.target_critic.weights)]\n",
    "        self.target_critic.set_weights(updates)\n",
    "        \n",
    "        \n",
    "    def choose_action(self, observation=None, training=False, explore=False):\n",
    "        \"\"\"\n",
    "        Return actions from the actor network\n",
    "        \"\"\"\n",
    "        \n",
    "        if not explore:\n",
    "            actions = self.actor(observation)\n",
    "            if training:\n",
    "                actions = tf.clip_by_value(\n",
    "                    tf.add(actions, tf.multiply(tf.random.normal(shape=actions.shape), \n",
    "                                                self.exploration_noise)), \n",
    "                    clip_value_min=self.min_action, \n",
    "                    clip_value_max=self.max_action)\n",
    "        else:\n",
    "            actions = tf.random.uniform(minval=self.min_action, maxval=self.max_action, shape=(1,self.n_actions))  \n",
    "            # technically tf.random.uniform does not include the upper bound but it practically does not make a difference\n",
    "        return actions\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def update(self, state_batch, next_state_batch, action_batch, reward_batch, done_batch):\n",
    "        \"\"\"\n",
    "        Update weights of the networks\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            noise = tf.clip_by_value(tf.multiply(tf.random.normal(shape=action_batch.shape), \n",
    "                                                 self.policy_noise), \n",
    "                                     clip_value_min=-self.noise_clip, \n",
    "                                     clip_value_max=self.noise_clip)\n",
    "            \n",
    "            target_action_batch = tf.clip_by_value(tf.add(self.target_actor(next_state_batch), noise),\n",
    "                                                   clip_value_min=self.min_action, \n",
    "                                                   clip_value_max=self.max_action)\n",
    "\n",
    "            target_Q1, target_Q2 = self.target_critic(next_state_batch, target_action_batch)\n",
    "            target_Q = tf.squeeze(tf.math.minimum(target_Q1, target_Q2), 1)\n",
    "            done_batch = tf.cast(done_batch, dtype=tf.float32)\n",
    "            reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "            target_Q = reward_batch + (1-done_batch) * self.gamma * target_Q\n",
    "            \n",
    "            current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n",
    "            current_Q1 = tf.squeeze(current_Q1, 1)\n",
    "            current_Q2 = tf.squeeze(current_Q2, 1)\n",
    "\n",
    "            critic_loss = keras.losses.MSE(current_Q1, target_Q) + keras.losses.MSE(current_Q2, target_Q)\n",
    "            \n",
    "        critic_network_gradient = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_network_gradient, self.critic.trainable_variables))\n",
    "\n",
    "        if self.total_steps % self.policy_freq == 0:\n",
    "            with tf.GradientTape() as tape:\n",
    "                actor_loss = tf.math.reduce_mean(-self.critic.Q1(state_batch, self.actor(state_batch)))\n",
    "\n",
    "            actor_network_gradient = tape.gradient(actor_loss,self.actor.trainable_variables)\n",
    "            self.actor.optimizer.apply_gradients(zip(actor_network_gradient, self.actor.trainable_variables))\n",
    "        \n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Perform a complete learning step\n",
    "        \"\"\"\n",
    "\n",
    "        self.total_steps += 1\n",
    "        if self.memory.mem_counter < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        (state_batch, next_state_batch, action_batch, \n",
    "         reward_batch, done_batch) = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        self.update(state_batch, next_state_batch, action_batch, reward_batch, done_batch)\n",
    "        self.update_network_parameters()        \n",
    "        \n",
    "        \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        \"\"\"\n",
    "        Store an experience into the experience replay buffer\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "        \n",
    "        \n",
    "    def save_models(self, game_num):\n",
    "        \"\"\"\n",
    "        Save models weights to the checkpoint directory\n",
    "        \"\"\"\n",
    "\n",
    "        self.actor.save_weights(f\"{self.ckpt_dir}/actor_game{game_num}.tf\")\n",
    "        self.target_actor.save_weights(f\"{self.ckpt_dir}/target_actor_game{game_num}.tf\")\n",
    "        self.critic.save_weights(f\"{self.ckpt_dir}/critic_game{game_num}.tf\")\n",
    "        self.target_critic.save_weights(f\"{self.ckpt_dir}/target_critic_game{game_num}.tf\")\n",
    "\n",
    "        \n",
    "    def load_models(self, game_num):\n",
    "        \"\"\"\n",
    "        Load models weights from the checkpoint directory\n",
    "        \"\"\"\n",
    "\n",
    "        self.actor.load_weights(f\"{self.ckpt_dir}/actor_game{game_num}.tf\")\n",
    "        self.target_actor.load_weights(f\"{self.ckpt_dir}/target_actor_game{game_num}.tf\")\n",
    "        self.critic.load_weights(f\"{self.ckpt_dir}/critic_game{game_num}.tf\")\n",
    "        self.target_critic.load_weights(f\"{self.ckpt_dir}/target_critic_game{game_num}.tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411b689",
   "metadata": {},
   "source": [
    "### Forex Spread-Betting Environment <a name='env'></a> \n",
    "#### [Details]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e471772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForexEnv:\n",
    "    \"\"\"\n",
    "    Forex Environment that the agent interacts with in training and validation\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 agent_obs_arrays,\n",
    "                 bidask_arrays,\n",
    "                 initial_balance=1,\n",
    "                 bid_col=3,\n",
    "                 ask_col=7,\n",
    "                 p=0.1,\n",
    "                 n_actions=1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        agent_obs_arrays: Agent observable arrays, representation of market's states, where agent_obs_arrays.shape = (timestamps, num_features)\n",
    "         \n",
    "\n",
    "        bidask_arrays: Unprocessed price data for reward calculation, must include Bid and Ask prices for reward calculation\n",
    "         \n",
    "\n",
    "        initial_balance : Initial balance available to the agent, default 1 (i.e. 100%)\n",
    "         \n",
    "\n",
    "        bid_col: Column of Bid price in pandas.DataFrame format or index of Bid price in numpy.array format, to be used for reward calculation, default 3\n",
    "        (A list of Bid columns can be pass when trading multiple currencies)\n",
    "         \n",
    "            e.g. bid_col = 3 when BidClose is used for reward calculation, where\n",
    "            bidask_arrays.shape = (timestamps, ['BidOpen', 'BidHigh', 'BidLow', 'BidClose',\n",
    "                                                'AskOpen', 'AskHigh', 'AskLow', 'AskClose'])\n",
    "\n",
    "        ask_col: Column of Ask price in pandas.DataFrame format or index of Bid price in numpy.array format, to be used for reward calculation, default 7\n",
    "        (A List of Ask columns can be pass when trading multiple currencies)\n",
    "\n",
    "            e.g. ask_col = 7 when AskClose is used for reward calculation, where\n",
    "            bidask_arrays.shape = (timestamps, ['BidOpen', 'BidHigh', 'BidLow', 'BidClose',\n",
    "                                                'AskOpen', 'AskHigh', 'AskLow', 'AskClose'])\n",
    "\n",
    "        p: p% of the entire balance that is controllable by the agent at each timestamp., default 0.1\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (tf.convert_to_tensor(agent_obs_arrays[:, np.newaxis], tf.float32),\n",
    "             tf.convert_to_tensor(bidask_arrays[:, np.newaxis], tf.float32)))\n",
    "        self.dataset_len = self.dataset.cardinality().numpy()\n",
    "        self.num_features = agent_obs_arrays.shape[-1] + n_actions + 1  \n",
    "        \n",
    "        self.initial_balance = initial_balance\n",
    "        self.bid_col = bid_col\n",
    "        self.ask_col = ask_col\n",
    "        self.p = p\n",
    "        self.n_actions = n_actions\n",
    "                    \n",
    "            \n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        Determines if the agent has arrived at a terminal state\n",
    "        \"\"\"\n",
    "        \n",
    "        return (self.current_pos==self.dataset_len) or (self.balance<0)\n",
    "    \n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Return the state of the timestamp\n",
    "        \"\"\"\n",
    "        \n",
    "        state, ba = self.iterator.get_next()\n",
    "        state = np.concatenate([self.current_action, state], axis=1)\n",
    "        self.current_pos += 1\n",
    "        \n",
    "        return state, ba\n",
    "\n",
    "    \n",
    "    def get_reward(self, reward_state, reward_state_):\n",
    "        \"\"\"\n",
    "        Reward calculation and agent's episodic balance update\n",
    "        \"\"\"\n",
    "        \n",
    "        short_reward = tf.reshape(tf.gather(reward_state_[0], indices=tf.constant(self.ask_col))\n",
    "                                  - tf.gather(reward_state[0], indices=tf.constant(self.bid_col)),\n",
    "                                  (1, -1))\n",
    "        long_reward = tf.reshape(tf.gather(reward_state_[0], indices=tf.constant(self.bid_col))\n",
    "                                  - tf.gather(reward_state[0], indices=tf.constant(self.ask_col)),\n",
    "                                  (1, -1))\n",
    "        reward = ((tf.cast(tf.less(self.current_action, 0), dtype=tf.float32) \n",
    "                   * self.current_action\n",
    "                   * 0.01 * self.p * self.balance \n",
    "                   * 10000 * short_reward) # 10000 here defines the change in reward given the pip_delta\n",
    "                  \n",
    "                  + (tf.cast(tf.greater(self.current_action, 0), dtype=tf.float32)\n",
    "                     * self.current_action\n",
    "                     * 0.01 * self.p * self.balance \n",
    "                     * 10000 * long_reward)) # 10000 here defines the change in reward given the pip_delta\n",
    "        \n",
    "        reward = tf.reshape(tf.reduce_sum(reward), (1, -1))\n",
    "        self.balance += reward\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    \n",
    "    def step(self, action, reward_state):\n",
    "        \"\"\"\n",
    "        Move to the next timestamp\n",
    "        \"\"\"\n",
    "        \n",
    "        self.current_action = action\n",
    "        state_, reward_state_ = self.get_state()\n",
    "        reward = self.get_reward(reward_state, reward_state_)\n",
    "        state_ = np.concatenate([self.balance, state_], axis=1)\n",
    "        \n",
    "        return state_, reward, self.is_done(), reward_state_, None\n",
    "    \n",
    "    \n",
    "    def reset(self, evaluate=False):\n",
    "        \"\"\"\n",
    "        Reset to a new episode\n",
    "        \"\"\"\n",
    "        \n",
    "        self.iterator = iter(self.dataset)\n",
    "        self.current_pos = 0\n",
    "        self.balance = tf.convert_to_tensor([[self.initial_balance]], dtype=tf.float32)\n",
    "        self.current_action = tf.zeros(shape=(1, self.n_actions), dtype=tf.float32)\n",
    "        if not evaluate:\n",
    "            steps = np.random.randint(low=1, high=self.dataset_len-2, \n",
    "                                      size=(1,), dtype=np.int32)\n",
    "            for step in range(int(steps)):\n",
    "                observation, reward_state = self.get_state()\n",
    "        else:\n",
    "            observation, reward_state = self.get_state()\n",
    "        observation = tf.concat([self.balance, observation], axis=1)\n",
    "        \n",
    "        return observation, reward_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc7d0c",
   "metadata": {},
   "source": [
    "### Set hypermarameters <a name=\"hparms\"></a>  \n",
    "#### [Training and Validation]\n",
    "#### **(User-input optional)**<br>  \n",
    "\n",
    "For details, refer to ```TD3``` class & ```ForexEnv``` class / explanation paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53593d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 2000             # Number of training games \n",
    "initial_balance = 1          # Initial balance of the agent\n",
    "n_actions = 1                # Number of action the agent takes at each timestamp\n",
    "max_action = 1               # Maximum amount of the p% balance the agent can long at each timestamp (e.g. 1, i.e. long 100%)\n",
    "min_action = -1              # Maximum amount of the p% balance the agent can short at each timestamp (e.g. -1, i.e. short 100%)\n",
    "bid_col = 3                  # The position of the Bid column in the bidask dataframe to use for reward calculation. \n",
    "                             # (For details, please refer to ForexEnv class / explanation paper / Define Agent-observable dataset)\n",
    "ask_col = 7                  # The position of the Ask column in the bidask dataframe to use for reward calculation. \n",
    "                             # (For details, please refer to ForexEnv class / explanation paper / Define Agent-observable dataset)\n",
    "gamma = 0.995                # Farsightedness, how much the agent values the future return\n",
    "tau = 0.002                  # Target networks update rate\n",
    "exploration_noise = 0.3      # Standard deviation of normally sampled noise added to actor's action in training\n",
    "p = 0.1                      # p% of the entire balance that is controllable by the agent at each timestamp\n",
    "policy_noise = 0.15          # Standard deviation of normally sampled noise added to target_actor's action batch\n",
    "noise_clip = 0.3             # Clip values of added noise to target_actor's action batch\n",
    "activation = 'relu'          # Hidden layers activation function in all actors and critics (keras dense) networks\n",
    "policy_freq = 4              # Policy update frequency\n",
    "fc_dim = 1024                # Number of nodes in each hidden dense layer\n",
    "ac_lr = 1e-5                 # Learning rate of the actors\n",
    "cr_lr = 1e-5                 # Learning rate of the critics\n",
    "batch_size = 64              # Batch size of each experience sample\n",
    "max_memory_size = 1000000    # Maximum replay buffer memory size\n",
    "uniform_action_steps = 10000 # Consecutive steps of uniformly sampled actions in the beginning of training\n",
    "num_actor_layers = 4         # Number of hidden layers in each actor networks\n",
    "num_critic_layers = 4        # Number of hidden layers in each critic networks\n",
    "eval_freq = 10               # Frequency of evaluation during training\n",
    "model_save_freq = 10         # Frequency of saving the model's weights\n",
    "hist_save_freq = 30          # Frequency of saving the history\n",
    "model_name = \"TD3\"           # Name of the model\n",
    "ckpt_name = \"ckpt\"           # Name of the model's weights checkpoint folder\n",
    "graph_name = \"graphs\"        # Name of the graphs folder\n",
    "hist_name = \"history\"        # Name of the history folder\n",
    "restore_ckpt_num = None      # Any checkpoint number to be restored from the checkpoint folder (TD3.load_models() will be called)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ffbe0",
   "metadata": {},
   "source": [
    "### Training and Validation <a name='train'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7bdfe0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ckpt_dir = f\"{model_name}/{ckpt_name}\"\n",
    "graph_dir = f\"{model_name}/{graph_name}\"\n",
    "hist_dir = f\"{model_name}/{hist_name}\"\n",
    "\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "if not os.path.exists(graph_dir):\n",
    "    os.makedirs(graph_dir)\n",
    "if not os.path.exists(hist_dir):\n",
    "    os.makedirs(hist_dir)\n",
    "\n",
    "history = {}\n",
    "\n",
    "train_env = ForexEnv(agent_obs_arrays=train_agent_obs, bidask_arrays=train_historical,\n",
    "                     initial_balance=initial_balance, bid_col=bid_col, \n",
    "                     ask_col=ask_col, p=p, n_actions=n_actions)\n",
    "\n",
    "val_env = ForexEnv(agent_obs_arrays=val_agent_obs, bidask_arrays=val_historical,\n",
    "                   initial_balance=initial_balance, bid_col=bid_col, \n",
    "                   ask_col=ask_col, p=p, n_actions=n_actions)\n",
    "\n",
    "agent = TD3(state_dim=train_env.num_features, n_actions=n_actions, gamma=gamma, \n",
    "            tau=tau, exploration_noise=exploration_noise, policy_noise=policy_noise, \n",
    "            noise_clip=noise_clip, activation=activation, policy_freq=policy_freq, fc_dim=fc_dim, \n",
    "            ac_lr=ac_lr, cr_lr=cr_lr, batch_size=batch_size, max_memory_size=max_memory_size, \n",
    "            uniform_action_steps=uniform_action_steps, num_actor_layers=num_actor_layers, \n",
    "            num_critic_layers=num_critic_layers, max_action=max_action, min_action=min_action, \n",
    "            ckpt_name=ckpt_name, model_name=model_name)\n",
    "\n",
    "if restore_ckpt_num is not None:\n",
    "    agent.update(np.ones((batch_size, train_env.num_features)),\n",
    "                 np.ones((batch_size, train_env.num_features)),\n",
    "                 np.ones((batch_size, n_actions)),\n",
    "                 np.ones((batch_size, )),\n",
    "                 np.ones((batch_size, )))\n",
    "    agent.update_network_parameters()\n",
    "    agent.load_models(restore_ckpt_num)\n",
    "    tbar = trange(restore_ckpt_num+1, restore_ckpt_num+num_games+1)\n",
    "    uniform_action_count = uniform_action_steps + 1\n",
    "    print(\"Checkpoint Restored\")\n",
    "\n",
    "else:\n",
    "    tbar = trange(1, num_games+1)\n",
    "    uniform_action_count = 0\n",
    "\n",
    "for i in tbar:\n",
    "    if (uniform_action_count > uniform_action_steps) & (i % eval_freq == 0):\n",
    "        evaluate = True\n",
    "        env = val_env\n",
    "    else:\n",
    "        evaluate = False\n",
    "        env = train_env\n",
    "    observation, reward_state = env.reset(evaluate=evaluate)\n",
    "    done = False\n",
    "    score_list = []\n",
    "    balance_list = []\n",
    "    starting_position = env.current_pos - 1\n",
    "    while not done:\n",
    "        if uniform_action_count <= uniform_action_steps:\n",
    "            action = agent.choose_action(explore=True)\n",
    "            uniform_action_count += 1\n",
    "        else:\n",
    "            action = agent.choose_action(observation=observation, training=not evaluate)\n",
    "        observation_, reward, done, reward_state_, info = env.step(action, reward_state)\n",
    "        score_list.append(np.reshape(reward, (1,))[0])\n",
    "        balance_list.append(np.reshape(env.balance, (1,))[0])\n",
    "        if not evaluate:\n",
    "            agent.remember(observation, action, reward, observation_, done)\n",
    "            agent.learn()\n",
    "        observation = observation_\n",
    "        reward_state = reward_state_\n",
    "        if env.current_pos % 200 == 0:\n",
    "            tbar.set_description(f\"Game {i} Current Position {env.current_pos}\\\n",
    "            Current Balance: {env.balance[0,0].numpy():.3f} Training Steps: {agent.total_steps}\")\n",
    "\n",
    "    history[f\"{'Val ' * evaluate}Game{i} Score\"] = score_list\n",
    "    history[f\"{'Val ' * evaluate}Game{i} Balance\"] = balance_list\n",
    "    \n",
    "    print(f\"{'Val ' * evaluate}Game {i} \\\n",
    "    Starting Position {starting_position} \\\n",
    "    Ending Position {env.current_pos} \\\n",
    "    Ending Balance {env.balance[0,0].numpy():.3f} \\\n",
    "    Avg_reward {(env.balance[0,0].numpy()) ** (1 / (env.current_pos - starting_position)) - 1}\")\n",
    "\n",
    "    if (i % model_save_freq == 0) & (uniform_action_count > uniform_action_steps):\n",
    "        agent.save_models(i)\n",
    "\n",
    "    if (i % hist_save_freq == 0):\n",
    "        with open(f\"{hist_dir}/history_up_to_game{i}.pkl\", 'wb') as f:\n",
    "            pickle.dump(history, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    if evaluate:\n",
    "        plt.plot(np.array(balance_list))\n",
    "        plt.xlabel(\"Timestamps\")\n",
    "        plt.ylabel(\"Balance\")\n",
    "        plt.title(f\"Val Game {i}\")\n",
    "        plt.savefig(fname=f\"{graph_dir}/Val Game {i}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37fd36",
   "metadata": {},
   "source": [
    "### Agent-Iteration Testing <font size=4>*(If validation results do not stablise)*</font>  <a name='test'></a>\n",
    "#### [Testing]\n",
    "#### **(User-input required)**\n",
    "User can evaluate any particular iterations of the model if the model's performance does not stabilise<br><br>  \n",
    "\n",
    "#### 1. Historical price dataset for testing\n",
    "```test_historical```, same requirements as the historical price dataset for training and validation, but will be used for testing.\n",
    "           \n",
    "<br><br>\n",
    "#### 2. An agent-observable dataset\n",
    "```test_agent_obs```, same requirements as the agent-observable dataset for training and validation, but will be used for testing<br><br>\n",
    "I. User can import custom agent-observable dataset by defining ```test_agent_obs```  \n",
    "- Ensure entries and exits timestamps (indices) in ```test_historical``` represent the same as which of the ```test_agent_obs```'s<br><br>\n",
    "\n",
    "II. User can build a ```test_agent_obs``` from ```test_historical```, the same way the ```agent_observable_data``` for training and validation is built by calling ```build_agent_obs_dataset()```.  \n",
    "- Note: the first 55 days trading days in ```test_historical``` will not be visible in the ```test_agent_obs```, due to preprocessing purposes.\n",
    "\n",
    "<br><br>\n",
    "#### 3. Load weights of the actor(s)\n",
    "The model's weights have to be restored to be tested<br><br>\n",
    "\n",
    "- User can load any particular iteration of the model by  \n",
    "    - defining ```restore_ckpt_dir``` (if not already defined during training), default ```ckpt_dir```\n",
    "    - and appending the actor checkpoint filename(s) within the checkpoint directory to the ```load_ckpts``` list  \n",
    "        - e.g. To restore ```\"./TD3/ckpt/actor_100.tf\"``` and ```\"./TD3/ckpt/actor_120.tf\"```  \n",
    "          ```ckpt_dir = \"./TD3/ckpt\"```    \n",
    "          ```load_ckpts = [\"actor_100.tf\", \"actor_120.tf\"]```<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d11c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Unprocessed dataset in testing environment for reward calculation (user-input)\n",
    "test_historical = \n",
    "\n",
    "# 2I. Agent-observable dataset in testing enviornment\n",
    "test_agent_obs = \n",
    "\n",
    "# 3.define checkpoints directory and ckeckpoints to load\n",
    "restore_ckpt_dir = ckpt_dir\n",
    "load_ckpts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664d9dfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_historical = np.array(test_historical.loc[test_agent_obs.index])\n",
    "test_agent_obs = np.array(test_agent_obs)\n",
    "    \n",
    "evaluate = True\n",
    "\n",
    "ai_eval_env = ForexEnv(agent_obs_arrays=test_agent_obs, bidask_arrays=test_historical,\n",
    "                       initial_balance=initial_balance, bid_col=bid_col, \n",
    "                       ask_col=ask_col, p=p, n_actions=n_actions)\n",
    "    \n",
    "ai_eval_actor = ActorNetwork(fc_dim=fc_dim, \n",
    "                             num_layers=num_actor_layers, \n",
    "                             activation=activation,\n",
    "                             n_actions=n_actions)\n",
    "\n",
    "\n",
    "ai_eval_history = {}\n",
    "for ckpt in load_ckpts:\n",
    "    name = ckpt.rsplit(\".\", 1)[0]\n",
    "    ai_eval_actor(tf.ones((1, ai_eval_env.num_features)))\n",
    "    ai_eval_actor.load_weights(f\"{restore_ckpt_dir}/{ckpt}\")\n",
    "    print(f\"Checkpoint {name} Restored\")\n",
    "    \n",
    "    observation, reward_state = ai_eval_env.reset(evaluate=evaluate)\n",
    "    done = False\n",
    "    score_list = []\n",
    "    balance_list = []\n",
    "    starting_position = ai_eval_env.current_pos - 1\n",
    "    while not done:\n",
    "        action = ai_eval_actor(observation)\n",
    "        observation_, reward, done, reward_state_, info = ai_eval_env.step(action, reward_state)\n",
    "        score_list.append(np.reshape(reward,(1,))[0])\n",
    "        balance_list.append(np.reshape(ai_eval_env.balance,(1,))[0])\n",
    "        observation = observation_\n",
    "        reward_state = reward_state_\n",
    "\n",
    "    ai_eval_history[f\"{name} Weights Test Score\"] = score_list\n",
    "    ai_eval_history[f\"{name} Weights Test Balance\"] = balance_list\n",
    "    print(f\"{name} Weights Test \\\n",
    "    Starting Position {starting_position} \\\n",
    "    Ending Position {ai_eval_env.current_pos} \\\n",
    "    Ending Balance {ai_eval_env.balance[0,0].numpy():.3f} \\\n",
    "    Avg_reward {(ai_eval_env.balance[0,0].numpy()) ** (1 / (ai_eval_env.current_pos - starting_position)) - 1}\")\n",
    "\n",
    "    plt.plot(np.array(balance_list))\n",
    "    plt.xlabel(\"Timestamps\")\n",
    "    plt.ylabel(\"Balance\")\n",
    "    plt.title(f\"{name} Test\")\n",
    "    plt.savefig(fname=f\"{graph_dir}/{name} Test.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9bf0f",
   "metadata": {},
   "source": [
    "### Simple implementation within the notebook <a name='simple_imp'></a>\n",
    "\n",
    "Restore a trained model and generate the estimated optimal actions given current data\n",
    "\n",
    "1. Load weights for an actor by defining restore_actor_ckpt_dir  \n",
    "e.g. To restore ```\"./TD3/ckpt/actor_100.tf\"```  \n",
    "```restore_actor_ckpt_dir = \"./TD3/ckpt\"```  \n",
    "```actor_ckpt = \"actor_100.tf\"```<br><br>  \n",
    "\n",
    "2. Input the observasions  \n",
    "    - Market observation at one timestamp (usually the last timestamp)  \n",
    "    e.g. ```market_observation = recent_df.iloc[-1]```<br><br>  \n",
    "\n",
    "3. Receive the model's estimated optimal action  \n",
    "    - The model (actor) estimates the optimal actions given the observation\n",
    "    - Then long `(+)` or short `(-)` ```p% * actions``` of current balance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf56ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to the actor's weights to be restored\n",
    "restore_actor_ckpt_dir = \n",
    "\n",
    "# Define the actor's weights to be restored\n",
    "actor_ckpt = \n",
    "\n",
    "# Define the current balance\n",
    "current_balance = \n",
    "\n",
    "# Define the opened trades/positions (actions at the last timestamp)\n",
    "previous_actions = \n",
    "\n",
    "# Define the observation of the market (last index in the agent-observable dataset)\n",
    "market_observation = \n",
    "\n",
    "\n",
    "current_balance = np.reshape(current_balance, (1,-1))\n",
    "previous_actions = np.reshape(previous_actions, (1, -1))\n",
    "market_observation = np.array(market_observation).reshape((1, -1))\n",
    "observation = np.concatenate([current_balance, previous_actions, market_observation], axis=-1)\n",
    "observation = tf.cast(observation, dtype=tf.float32)\n",
    "\n",
    "# Create the actor\n",
    "final_actor = ActorNetwork(fc_dim=fc_dim, \n",
    "                           num_layers=num_actor_layers, \n",
    "                           activation=activation,\n",
    "                           n_actions=n_actions)\n",
    "\n",
    "# Weights will be initialised\n",
    "_ = final_actor(observation)\n",
    "\n",
    "# Load weights\n",
    "final_actor.load_weights(f\"{restore_actor_ckpt_dir}/{actor_ckpt}\")\n",
    "\n",
    "# Receive output\n",
    "actions = final_actor(observation)\n",
    "print(\"Estimated optimal action:\", actions[0].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
